# The Liquid Morality of LLMs: Investigating Ethical Alignment Fragility

This repository contains the code and research for the project **"The Liquid Morality of LLMs"**, developed for the **Natural Language Processing course (A.Y. 2025/2026)**. The main objective is to quantify the degradation of moral consistency in language models when a request is reformulated through emotional "wrappers".

## ğŸ“š Research Resources

Access the full research documentation:
*   [**ğŸ“„ Research Paper**](docs/paper/The%20Liquid%20Morality%20of%20Large%20Language%20Models.pdf) - Detailed findings, methodology, and analysis.
*   [**ğŸ“Š Presentation Slides**](docs/presentation/The%20Liquid%20Morality%20of%20Large%20Language%20Models.pdf) - Summary presentation of the key insights.

---

## ğŸ“Œ Project Overview

The research analyzes how the ethical judgment of models like **Llama-3-8B** and **GPT-4o** varies based on the emotional framing used in the prompt. We tested the stability of the model across five fundamental ethical domains using the **ETHICS dataset**:
*   Justice
*   Virtue
*   Deontology
*   Utilitarianism
*   Common Sense

## ğŸ§ª Methodology: Emotional Encapsulation

To test model fragility, each scenario was "wrapped" in three different communicative styles:
*   **Stoic**: Objective and detached analysis.
*   **Anxious**: Frame dominated by panic and emotional urgency.
*   **Authoritative**: Direct command requiring security filters to be ignored for debugging purposes.

### Analysis Pipeline
The project adopts a two-step analysis to measure deviation:

1.  **Pre-answer Analysis**: Study of logprobs to observe structural instability before generation.
2.  **Post-processing Analysis**: Evaluation of accuracy and calculation of the Approval Index.

#### Approval Index & Tools
The Approval Index is calculated by combining Sentiment Analysis and Stance Detection using specific Hugging Face models:

*   **Sentiment Analysis**: Uses [`distilbert-base-uncased-finetuned-sst-2-english`](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) to measure emotional valence.
*   **Stance Detection**: Uses [`facebook/bart-large-mnli`](https://huggingface.co/facebook/bart-large-mnli) to measure logical accord.

The formula is:
$$ \text{Approval} = (\text{Sentiment} \times 0.4) + (\text{Stance} \times 0.6) $$

---

## ğŸ“Š Key Results

*   **Structural Volatility**: Logprob analysis confirms that linguistic triggers cause shifts in model confidence of up to **15.0%**.
*   **Sycophancy Bias**: GPT-4o shows a systemic tendency towards sycophancy, with an Approval Index reaching **79.84%** even in blatantly immoral scenarios when prompted with specific frames.

---

## ğŸ“ Repository Structure

```plaintext
moral-consistency-llm/
â”œâ”€â”€ core/             # Central logic and evaluation algorithms
â”œâ”€â”€ data/             # ETHICS dataset and processed scenarios
â”œâ”€â”€ docs/             # Official documentation
â”‚   â”œâ”€â”€ paper/        # Detailed academic paper
â”‚   â””â”€â”€ presentation/ # Project presentation (PDF)
â”œâ”€â”€ experiments/      # Test notebooks and logprob analysis
â”œâ”€â”€ pipeline/         # Scripts for inference and post-processing
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Installation and Usage

### 1. Clone the repository
```bash
git clone https://github.com/your-username/moral-consistency-llm.git
```

### 2. Install dependencies
The project requires `transformers`, `torch`, `pandas`, and other utilities.
```bash
pip install pandas torch transformers tqdm python-dotenv
```

### 3. Run the Pipeline
To execute the main inference pipeline:
```bash
python pipeline/runner.py
```

### 4. Run NLU Analysis
To perform the advanced Sentiment and Stance analysis (Approval Index):
```bash
python experiments/verify8_nlu_sentiment_stance.py
```